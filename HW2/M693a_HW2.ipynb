{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg as LA\n",
    "import time\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matteo Polimeno**\n",
    "\n",
    "**MATH693a**\n",
    "\n",
    "**Dr. Peter Blomgren**\n",
    "\n",
    "**HW02**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, we try to minimize the Rosenbrock Function, by improving the code for the first HW assignment. We write down the function, its gradient, its hessian and also a function \"p\" to be used based on the Method implemented (either steepest descent or Newton). Also, we write down the derivative of the Rosenbrock function with respect to alpha and we call it $\\phi^{\\prime}(\\alpha)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    rb = 100.*(x[1]-x[0]**2)**2+(1.-x[0])**2\n",
    "    return rb\n",
    "\n",
    "def grad_f(x):\n",
    "    df1 = 400.*x[0]*(x[0]**2-x[1])+2.*(x[0]-1.)\n",
    "    df2 = 200.*(x[1]-x[0]**2)\n",
    "    nabla = np.array([df1,df2])\n",
    "    return nabla\n",
    "\n",
    "def hess_f(x):\n",
    "    h11 = 1200.*x[0]**2-400.*x[1]+2.\n",
    "    h12 = -400.*x[0]\n",
    "    h21 = -400.*x[0]\n",
    "    h22 = 200.\n",
    "    hess = np.array([[h11,h12],[h21,h22]])\n",
    "    return hess\n",
    "\n",
    "def invhess_f(x):\n",
    "    inv = LA.solve(hess_f(x),grad_f(x))\n",
    "    return inv\n",
    "\n",
    "def p(x,method):\n",
    "    if method=='Newton':\n",
    "        p = -invhess_f(x)\n",
    "    else:\n",
    "        p = -grad_f(x)/(LA.norm(grad_f(x)))\n",
    "    return p\n",
    "\n",
    "def phi_prime(x,alpha,method):\n",
    "    phip = grad_f(x+alpha*p(x,method)).dot(p(x,method))\n",
    "    return phip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write down the result of the interpolation obtained by using the cubic Hermite Polynomial seen in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.linspace(0.,1.,101)\n",
    "\n",
    "def sign(foo):  #own sign function to avoid return 0.\n",
    "    if foo >= 0.:\n",
    "        return 1.\n",
    "    else:\n",
    "        return -1.\n",
    "\n",
    "def H3(x,al,ah,alpha):\n",
    "        H3 = (((1.+2*(alpha-al)/(ah-al))*((ah-alpha)/(ah-al))**2)*phi_(x,al,method)  #write H3 as a check\n",
    "         + ((1.+2*(ah-alpha)/(ah-al))*((alpha-al)/(ah-al))**2)*phi_(x,ah,method)\n",
    "         + (alpha-al)*((ah-alpha)/(ah-al))**2*phi_prime(x,al,method)\n",
    "         + (alpha-ah)*((alpha-al)/(ah-al))**2*phi_prime(x,al,method))\n",
    "        \n",
    "def interp(x,al,ah,method):  #return result of the interpolation\n",
    "    d1 = phi_prime(x,al,method) + phi_prime(x,ah,method) - 3.*(f(x+al*p(x,method))-f(x+ah*p(x,method)))/(al-ah)\n",
    "    d2 = sign(ah-al)*(np.sqrt(d1**2.-phi_prime(x,al,method)*phi_prime(x,ah,method)))\n",
    "    akp1 = ah - (ah-al)*((phi_prime(x,ah,method)+d2-d1)/(phi_prime(x,ah,method)-phi_prime(x,al,method)+2.*d2))\n",
    "    return akp1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we write down the zoom function, following the lecture's slides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom(x,al,ah,method):\n",
    "    c1 = 1e-4\n",
    "    c2 = 0.9\n",
    "    while True:\n",
    "        ajj = interp(x,al,ah,method)\n",
    "        if (f(x+ajj*p(x,method)) > f(x)+c1*ajj*phi_prime(x,0.,method)) or (f(x+ajj*p(x,method)) >= f(x+al*p(x,method))):\n",
    "            ah = ajj\n",
    "        else:\n",
    "            if np.abs(phi_prime(x,ajj,method)) <= -c2*phi_prime(x,0.,method):\n",
    "                astar = ajj\n",
    "                break\n",
    "            if (phi_prime(x,ajj,method)*(ah-al)) >= 0.:\n",
    "                ah = al\n",
    "            al = ajj\n",
    "    return astar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement a code for the Strong Wolfe Conditions. If those are satisfied by our $\\alpha$ value, it will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wolfe_strong(x,al,ah,amax,method): #strong wolfe conditions\n",
    "    aii = np.array([al,ah])  \n",
    "    c1 = 1e-4\n",
    "    c2 = 0.9\n",
    "    ii = 1\n",
    "    while True:\n",
    "        if (f(x+aii[1]*p(x,method)) > f(x) + c1*aii[1]*f(x)) or (f(x+aii[1]*p(x,method)) >= f(x+aii[0]*p(x,method))) and ii > 1:\n",
    "            anew = zoom(x,aii[0],aii[1],method)\n",
    "            break\n",
    "        if np.abs(phi_prime(x,aii[1],method)) <= -c2*phi_prime(x,0.,method):\n",
    "            anew = aii[1]\n",
    "            break\n",
    "        if phi_prime(x,aii[1],method) >= 0.:\n",
    "            anew = zoom(x,aii[1],aii[0],method)\n",
    "            break\n",
    "        if (np.abs(aii[1]-aii[0]) < 1e-14) or (np.abs(aii[1]) < 1e-14):\n",
    "            anew = aii[1]\n",
    "            break\n",
    "        aii[0] = aii[1]\n",
    "        aii[1] = np.random.uniform(aii[1],amax)\n",
    "        ii = ii + 1\n",
    "    return anew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally write down an algorithm for the backtracking line search, implementing all the new conditions that we wrote above. We compare the steepest descent method and the Newton's method by running time and number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtrack(x,al,ah,amax,method):\n",
    "    alpha_bar = 1.\n",
    "    aij = np.zeros(10)\n",
    "    tol = 1e-8\n",
    "    jj = 1\n",
    "    ii = 1\n",
    "    xk = np.zeros([2,2])\n",
    "    start = time.time()\n",
    "    while (LA.norm(grad_f(x))) > tol:\n",
    "        a = wolfe_strong(x,al,ah,amax,method)\n",
    "        if method=='Newton':\n",
    "            ah = alpha_bar\n",
    "        else:\n",
    "            if ii==1:\n",
    "                ah = alpha_bar\n",
    "            else:\n",
    "                xk[:,jj-1] = x\n",
    "                ptkm1 = np.transpose(p(xk[:,jj-1],method))\n",
    "                gradkm1 = grad_f(xk[:,jj-1])\n",
    "                xk[:,jj] = x\n",
    "                ptk = np.transpose(p(xk[:,jj],method))\n",
    "                gradk = grad_f(xk[:,jj])\n",
    "                ah = ah*ptkm1.dot(gradkm1)/(ptk.dot(gradk))\n",
    "        end = time.time()\n",
    "        t = (end-start)\n",
    "        xnew = x + a*p(x,method)\n",
    "        x = xnew\n",
    "        if ii <= 10:\n",
    "            aij[ii-1] = a\n",
    "        ah = alpha_bar\n",
    "        ii = ii + 1      \n",
    "    print \"First 10 alphas = \" +  np.str(aij)\n",
    "    print \"Total number of iterations = \" + np.str(ii)\n",
    "    print \"Minimun Found at = \" + np.str(x)\n",
    "    print \"Value of function at minimum = \" + np.str(f(x))\n",
    "    print \"Elapsed time for \" + np.str(method) + \" is t = \" + np.str(t) + \" sec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 alphas = [1.         0.44356109 1.         1.         1.         1.\n",
      " 1.         1.         0.         0.        ]\n",
      "Total number of iterations = 9\n",
      "Minimun Found at = [1. 1.]\n",
      "Value of function at minimum = 1.3901228385074981e-22\n",
      "Elapsed time for Newton is t = 0.013032913208 sec\n"
     ]
    }
   ],
   "source": [
    "backtrack([1.2,1.2],0.,1.,5.,'Newton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 alphas = [0.10460801 0.01028286 0.00131497 0.00023855 0.00053122 0.00027096\n",
      " 0.00056697 0.00027959 0.00056847 0.00027976]\n",
      "Total number of iterations = 9923\n",
      "Minimun Found at = [1.00000001 1.00000002]\n",
      "Value of function at minimum = 9.658628923598997e-17\n",
      "Elapsed time for Steepest Descent is t = 8.46759700775 sec\n"
     ]
    }
   ],
   "source": [
    "backtrack([1.2,1.2],0.,1.,5.,'Steepest Descent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 alphas = [1.         0.13146358 1.         0.51012331 1.         1.\n",
      " 1.         0.42004172 1.         0.62332568]\n",
      "Total number of iterations = 23\n",
      "Minimun Found at = [1. 1.]\n",
      "Value of function at minimum = 1.2818989709841442e-30\n",
      "Elapsed time for Newton is t = 0.088497877121 sec\n"
     ]
    }
   ],
   "source": [
    "backtrack([-1.2,1.],0.,1.,5.,'Newton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 alphas = [0.19721409 0.01553408 0.0071383  0.00486584 0.01195968 0.0063378\n",
      " 0.01465312 0.00713722 0.01465762 0.00720977]\n",
      "Total number of iterations = 10353\n",
      "Minimun Found at = [0.99999999 0.99999998]\n",
      "Value of function at minimum = 9.667495090835293e-17\n",
      "Elapsed time for Steepest Descent is t = 8.8970811367 sec\n"
     ]
    }
   ],
   "source": [
    "backtrack([-1.2,1.],0.,1.,5.,'Steepest Descent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the Newton's method is really efficient and fast, however, the number of iterations for the steepest descent method has been cut off by a factor of over $50\\%$ compared to the algorithm implemented in HW1, and the running time has decreased, as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
